{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a51a449",
   "metadata": {},
   "source": [
    "\n",
    "# Job–CV Matching (All-in-One) — **SBERT + FAISS** + 5-Fold CV + 8 Methods\n",
    "\n",
    "**Điểm mới**: SBERT thật (multilingual) + FAISS thật, đủ 3 cải tiến **C6, C7, C8**, classification trước ranking, GroupKFold theo `job_id`, bộ metrics P@K, R@K, MAP, MRR, nDCG@K.  \n",
    "Notebook có fallback nếu thiếu thư viện.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e469e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.1-cp311-cp311-win_amd64.whl.metadata (111 kB)\n",
      "     ---------------------------------------- 0.0/111.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/111.1 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/111.1 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 30.7/111.1 kB 262.6 kB/s eta 0:00:01\n",
      "     ------------- ----------------------- 41.0/111.1 kB 281.8 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 61.4/111.1 kB 409.6 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 61.4/111.1 kB 409.6 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 61.4/111.1 kB 409.6 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 102.4/111.1 kB 295.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 111.1/111.1 kB 323.3 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\admin\\downloads\\cv-matching-system\\venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\downloads\\cv-matching-system\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\downloads\\cv-matching-system\\venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\downloads\\cv-matching-system\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\downloads\\cv-matching-system\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.5-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.1 MB 667.8 kB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/8.1 MB 660.6 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.1/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.1/8.1 MB 657.1 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.2/8.1 MB 930.9 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.2/8.1 MB 850.1 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.3/8.1 MB 820.5 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.3/8.1 MB 955.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.3/8.1 MB 832.3 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.6/8.1 MB 967.0 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.6/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.7/8.1 MB 976.7 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.8/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.8/8.1 MB 1.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.9/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.9/8.1 MB 1.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.9/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.1/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.2/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.2/8.1 MB 1.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.4/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.5/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.5/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.5/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 1.7/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 1.7/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.8/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.8/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.8/8.1 MB 1.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.0/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.0/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.2/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.2/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 1.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.5/8.1 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.6/8.1 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.6/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.8/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.8/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.8/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 2.9/8.1 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 3.0/8.1 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 3.0/8.1 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 3.1/8.1 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 3.2/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.3/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.3/8.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.6/8.1 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.6/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.7/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.7/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.7/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.8/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.8/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.9/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.9/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 4.0/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.1/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.1/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.1/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.2/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.2/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 4.2/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.3/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.3/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.4/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 4.4/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.6/8.1 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.6/8.1 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 4.6/8.1 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.8/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.8/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.8/8.1 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 4.9/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.9/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.9/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.0/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.1/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.1/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.1/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.3/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.3/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.4/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.4/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 5.4/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.5/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.5/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.6/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.6/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 5.6/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.7/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.9/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.9/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 5.9/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 6.1/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 6.1/8.1 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 6.1/8.1 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.1/8.1 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.1/8.1 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 985.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 984.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 984.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 984.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 963.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.2/8.1 MB 962.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.3/8.1 MB 958.8 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.3/8.1 MB 958.0 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.3/8.1 MB 954.4 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.3/8.1 MB 954.6 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.4/8.1 MB 956.3 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.5/8.1 MB 958.8 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.5/8.1 MB 955.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.5/8.1 MB 952.3 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.5/8.1 MB 954.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.1 MB 949.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.6/8.1 MB 950.1 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.7/8.1 MB 948.8 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.7/8.1 MB 948.1 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.7/8.1 MB 948.1 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.1 MB 937.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.1 MB 937.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.1 MB 937.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.1 MB 937.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.1 MB 937.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.7/8.1 MB 906.0 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.1 MB 901.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.1 MB 900.1 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.1 MB 900.0 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.9/8.1 MB 899.8 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.9/8.1 MB 901.1 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 7.0/8.1 MB 904.9 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 7.0/8.1 MB 904.9 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 905.4 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 903.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 907.6 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 908.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 909.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 909.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.1 MB 906.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.1 MB 903.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.1 MB 906.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.5/8.1 MB 905.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.1 MB 910.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.1 MB 910.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 911.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 911.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.7/8.1 MB 906.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/8.1 MB 908.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.8/8.1 MB 910.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.8/8.1 MB 913.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 906.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.1 MB 908.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.1 MB 911.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.1 MB 907.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 909.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 912.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 912.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 912.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 912.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 892.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 888.2 kB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "   ---------------------------------------- 0.0/225.2 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/225.2 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 41.0/225.2 kB 960.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 102.4/225.2 kB 837.8 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 143.4/225.2 kB 847.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 143.4/225.2 kB 847.9 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 184.3/225.2 kB 740.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 225.2/225.2 kB 723.9 kB/s eta 0:00:00\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.3 MB 1.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/2.3 MB 1.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.3 MB 901.1 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.3 MB 952.6 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.3 MB 926.0 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/2.3 MB 846.9 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/2.3 MB 846.9 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.3 MB 714.4 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.3 MB 714.4 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.3 MB 701.4 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.3 MB 675.6 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.4/2.3 MB 694.6 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.3 MB 707.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.3 MB 688.3 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.3 MB 731.4 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.6/2.3 MB 770.9 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.6/2.3 MB 788.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/2.3 MB 798.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/2.3 MB 798.8 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.7/2.3 MB 790.3 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.7/2.3 MB 790.3 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.3 MB 768.0 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.3 MB 768.0 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.3 MB 730.1 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.3 MB 726.9 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.3 MB 734.6 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.3 MB 722.4 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.9/2.3 MB 719.9 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 716.6 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 707.8 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 707.8 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 690.7 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.1/2.3 MB 696.3 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.3 MB 688.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.3 MB 688.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.3 MB 688.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.3 MB 655.3 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.1/2.3 MB 649.2 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.2/2.3 MB 655.4 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.2/2.3 MB 649.5 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.2/2.3 MB 655.4 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.2/2.3 MB 644.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.3/2.3 MB 660.5 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.4/2.3 MB 665.4 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.4/2.3 MB 670.3 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.3 MB 679.7 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.5/2.3 MB 674.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.3 MB 688.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.3 MB 688.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.3 MB 688.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.3 MB 688.0 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.5/2.3 MB 646.5 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.6/2.3 MB 642.6 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.3 MB 646.8 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.6/2.3 MB 651.2 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.3 MB 651.1 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.3 MB 659.1 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.3 MB 651.3 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 655.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.3 MB 659.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.3 MB 658.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.3 MB 662.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.3 MB 658.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.3 MB 651.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.3 MB 662.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.3 MB 655.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.3 MB 661.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 665.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.3 MB 671.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 668.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 674.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 658.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.3 MB 658.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 664.3 kB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "   ---------------------------------------- 0.0/73.8 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/73.8 kB ? eta -:--:--\n",
      "   -------------------------------------- - 71.7/73.8 kB 975.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 73.8/73.8 kB 676.6 kB/s eta 0:00:00\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.1 kiwisolver-1.4.9 matplotlib-3.10.5 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\Admin\\Downloads\\cv-matching-system\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] faiss unavailable. pip install faiss-cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install matplotlib\n",
    "\n",
    "# ==== Environment & Imports ====\n",
    "# !pip install faiss-cpu sentence-transformers  # nếu thiếu thư viện\n",
    "\n",
    "import os, math, random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "SBERT_OK = True\n",
    "FAISS_OK = True\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SBERT_OK = False\n",
    "    print(\"[WARN] sentence-transformers unavailable. pip install sentence-transformers\")\n",
    "try:\n",
    "    import faiss  # type: ignore\n",
    "except Exception:\n",
    "    FAISS_OK = False\n",
    "    print(\"[WARN] faiss unavailable. pip install faiss-cpu\")\n",
    "\n",
    "random.seed(42); np.random.seed(42)\n",
    "DATA_DIR = \"./data\"\n",
    "RESULTS_CSV_PATH = \"/mnt/data/match_results_summary_sbert_faiss.csv\"\n",
    "SBERT_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb64cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Ontology & Utils ====\n",
    "SKILL_ONTO = {\n",
    "    \"python\":[\"python\"], \"java\":[\"java\"], \"javascript\":[\"javascript\",\"js\"],\n",
    "    \"typescript\":[\"typescript\",\"ts\"], \"c#\":[\"c#\",\"csharp\"], \"cpp\":[\"c++\",\"cpp\"],\n",
    "    \"php\":[\"php\"], \"go\":[\"golang\",\"go\"], \"html\":[\"html\"], \"css\":[\"css\"],\n",
    "    \"react\":[\"react\",\"react.js\",\"reactjs\"], \"vue\":[\"vue\",\"vue.js\",\"nuxt\"],\n",
    "    \"node.js\":[\"node\",\"node.js\",\"nodejs\",\"express\"], \"django\":[\"django\"], \"flask\":[\"flask\"], \"spring\":[\"spring\"],\n",
    "    \"sql\":[\"sql\"], \"nosql\":[\"mongodb\",\"cassandra\",\"dynamodb\"],\n",
    "    \"aws\":[\"aws\"], \"gcp\":[\"gcp\"], \"azure\":[\"azure\"],\n",
    "    \"docker\":[\"docker\"], \"kubernetes\":[\"kubernetes\",\"k8s\"],\n",
    "    \"spark\":[\"spark\"], \"hadoop\":[\"hadoop\"],\n",
    "    \"communication\":[\"communication\",\"presentation\"],\n",
    "    \"teamwork\":[\"teamwork\",\"collaboration\"],\n",
    "    \"problem-solving\":[\"problem solving\",\"analytical\"]\n",
    "}\n",
    "TECH_GROUP = set([\"python\",\"java\",\"javascript\",\"typescript\",\"c#\",\"cpp\",\"php\",\"go\",\"html\",\"css\",\"react\",\"vue\",\"node.js\",\"django\",\"flask\",\"spring\",\"sql\",\"nosql\",\"aws\",\"gcp\",\"azure\",\"docker\",\"kubernetes\",\"spark\",\"hadoop\"])\n",
    "SOFT_GROUP = set([\"communication\",\"teamwork\",\"problem-solving\"])\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return \" \".join(str(s).lower().strip().split())\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b: return 0.0\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "def keyword_overlap(job_terms: set, cv_terms: set) -> float:\n",
    "    if not job_terms or not cv_terms: return 0.0\n",
    "    return len(job_terms & cv_terms) / max(1, len(job_terms))\n",
    "\n",
    "def exp_gap_penalty(required_years: int, cand_years: int) -> float:\n",
    "    gap = max(0, required_years - cand_years)\n",
    "    return gap / (required_years + 1e-6)\n",
    "\n",
    "def group_weighted_skill_score(job_skills: set, cv_skills: set) -> float:\n",
    "    inter = job_skills & cv_skills\n",
    "    if not inter: return 0.0\n",
    "    score = 0.0\n",
    "    for s in inter:\n",
    "        if s in TECH_GROUP: score += 1.0\n",
    "        elif s in SOFT_GROUP: score += 0.3\n",
    "        else: score += 0.5\n",
    "    return score / (len(job_skills) + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741e8d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV not found, generating synthetic data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>required_exp</th>\n",
       "      <th>industry</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J0000</td>\n",
       "      <td>Fullstack Engineer</td>\n",
       "      <td>html kubernetes node.js problem-solving sql he...</td>\n",
       "      <td>[html, kubernetes, node.js, problem-solving, sql]</td>\n",
       "      <td>7</td>\n",
       "      <td>ecommerce</td>\n",
       "      <td>remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J0001</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>azure cpp django go problem-solving python gaming</td>\n",
       "      <td>[azure, cpp, django, go, problem-solving, python]</td>\n",
       "      <td>10</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>hanoi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  job_id               title  \\\n",
       "0  J0000  Fullstack Engineer   \n",
       "1  J0001       Data Engineer   \n",
       "\n",
       "                                         description  \\\n",
       "0  html kubernetes node.js problem-solving sql he...   \n",
       "1  azure cpp django go problem-solving python gaming   \n",
       "\n",
       "                                     required_skills  required_exp  \\\n",
       "0  [html, kubernetes, node.js, problem-solving, sql]             7   \n",
       "1  [azure, cpp, django, go, problem-solving, python]            10   \n",
       "\n",
       "     industry location  \n",
       "0   ecommerce   remote  \n",
       "1  healthcare    hanoi  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>skills</th>\n",
       "      <th>experience_years</th>\n",
       "      <th>industry</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00000</td>\n",
       "      <td>hadoop kubernetes php spring experienced projects</td>\n",
       "      <td>[hadoop, kubernetes, php, spring]</td>\n",
       "      <td>3</td>\n",
       "      <td>education</td>\n",
       "      <td>remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00001</td>\n",
       "      <td>c# communication django kubernetes php react t...</td>\n",
       "      <td>[c#, communication, django, kubernetes, php, r...</td>\n",
       "      <td>0</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>hcmc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cv_id                                           raw_text  \\\n",
       "0  C00000  hadoop kubernetes php spring experienced projects   \n",
       "1  C00001  c# communication django kubernetes php react t...   \n",
       "\n",
       "                                              skills  experience_years  \\\n",
       "0                  [hadoop, kubernetes, php, spring]                 3   \n",
       "1  [c#, communication, django, kubernetes, php, r...                 0   \n",
       "\n",
       "     industry location  \n",
       "0   education   remote  \n",
       "1  healthcare     hcmc  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ==== Data Loader & Synthetic ====\n",
    "def generate_synthetic(n_jobs=100, n_cvs=500):\n",
    "    industries = [\"fintech\",\"ecommerce\",\"healthcare\",\"education\",\"gaming\"]\n",
    "    locations = [\"hanoi\",\"danang\",\"hcmc\",\"remote\",\"bangkok\"]\n",
    "    all_skills = list(SKILL_ONTO.keys())\n",
    "\n",
    "    jobs = []\n",
    "    for jid in range(n_jobs):\n",
    "        req_skills = sorted(set(np.random.choice(all_skills, size=np.random.randint(3,7), replace=False)))\n",
    "        jobs.append({\n",
    "            \"job_id\": f\"J{jid:04d}\",\n",
    "            \"title\": f\"{np.random.choice(['Backend','Frontend','Fullstack','Data','DevOps'])} Engineer\",\n",
    "            \"description\": \" \".join(req_skills) + \" \" + np.random.choice(industries),\n",
    "            \"required_skills\": req_skills,\n",
    "            \"required_exp\": int(np.random.choice([0,1,2,3,4,5,7,10])),\n",
    "            \"industry\": np.random.choice(industries),\n",
    "            \"location\": np.random.choice(locations),\n",
    "        })\n",
    "    jobs = pd.DataFrame(jobs)\n",
    "\n",
    "    cvs = []\n",
    "    for cid in range(n_cvs):\n",
    "        cv_skills = sorted(set(np.random.choice(all_skills, size=np.random.randint(4,10), replace=False)))\n",
    "        years = int(np.random.choice([0,1,2,3,4,5,7,10]))\n",
    "        cvs.append({\n",
    "            \"cv_id\": f\"C{cid:05d}\",\n",
    "            \"raw_text\": \" \".join(cv_skills) + \" experienced projects\",\n",
    "            \"skills\": cv_skills,\n",
    "            \"experience_years\": years,\n",
    "            \"industry\": np.random.choice(industries),\n",
    "            \"location\": np.random.choice(locations),\n",
    "        })\n",
    "    cvs = pd.DataFrame(cvs)\n",
    "    return jobs, cvs\n",
    "\n",
    "def load_or_generate():\n",
    "    jobs_path = os.path.join(DATA_DIR, \"jobs.csv\")\n",
    "    cvs_path = os.path.join(DATA_DIR, \"cvs.csv\")\n",
    "    if os.path.exists(jobs_path) and os.path.exists(cvs_path):\n",
    "        jobs = pd.read_csv(jobs_path)\n",
    "        cvs = pd.read_csv(cvs_path)\n",
    "        print(\"Loaded real CSVs from\", DATA_DIR)\n",
    "    else:\n",
    "        print(\"CSV not found, generating synthetic data...\")\n",
    "        jobs, cvs = generate_synthetic()\n",
    "    return jobs, cvs\n",
    "\n",
    "jobs, cvs = load_or_generate()\n",
    "display(jobs.head(2)); display(cvs.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847c4b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000,\n",
       "   job_id   cv_id           job_title  \\\n",
       " 0  J0000  C00361  Fullstack Engineer   \n",
       " 1  J0000  C00073  Fullstack Engineer   \n",
       " \n",
       "                                             job_desc  \\\n",
       " 0  html kubernetes node.js problem-solving sql he...   \n",
       " 1  html kubernetes node.js problem-solving sql he...   \n",
       " \n",
       "                                           job_skills  job_reqexp job_industry  \\\n",
       " 0  [html, kubernetes, sql, problem-solving, node.js]           7    ecommerce   \n",
       " 1  [html, kubernetes, sql, problem-solving, node.js]           7    ecommerce   \n",
       " \n",
       "   job_location                                            cv_text  \\\n",
       " 0       remote  c# communication cpp docker flask spring sql v...   \n",
       " 1       remote  django docker gcp hadoop nosql problem-solving...   \n",
       " \n",
       "                                            cv_skills  cv_exp cv_industry  \\\n",
       " 0  [c#, spring, sql, cpp, docker, vue, communicat...       3     fintech   \n",
       " 1  [django, gcp, sql, docker, problem-solving, no...       7   ecommerce   \n",
       " \n",
       "   cv_location  label_gt1  \n",
       " 0        hcmc          0  \n",
       " 1     bangkok          1  )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==== Build pairs & Ground Truth ====\n",
    "def build_pairs(jobs: pd.DataFrame, cvs: pd.DataFrame, max_pairs_per_job=150):\n",
    "    rows = []\n",
    "    for _, j in jobs.iterrows():\n",
    "        sampled = cvs.sample(min(max_pairs_per_job, len(cvs)), random_state=42)\n",
    "        jskills = set(j[\"required_skills\"]) if isinstance(j[\"required_skills\"], (list,set)) else set(str(j[\"required_skills\"]).split())\n",
    "        for _, c in sampled.iterrows():\n",
    "            cskills = set(c[\"skills\"]) if isinstance(c[\"skills\"], (list,set)) else set(str(c[\"skills\"]).split())\n",
    "            label_gt1 = 1 if len(jskills & cskills) >= 2 else 0\n",
    "            rows.append({\n",
    "                \"job_id\": j[\"job_id\"], \"cv_id\": c[\"cv_id\"],\n",
    "                \"job_title\": j[\"title\"], \"job_desc\": j[\"description\"],\n",
    "                \"job_skills\": list(jskills), \"job_reqexp\": j[\"required_exp\"],\n",
    "                \"job_industry\": j[\"industry\"], \"job_location\": j[\"location\"],\n",
    "                \"cv_text\": c[\"raw_text\"],\n",
    "                \"cv_skills\": list(cskills), \"cv_exp\": c[\"experience_years\"],\n",
    "                \"cv_industry\": c[\"industry\"], \"cv_location\": c[\"location\"],\n",
    "                \"label_gt1\": label_gt1,\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pairs = build_pairs(jobs, cvs, max_pairs_per_job=120)\n",
    "len(pairs), pairs.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dff75f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== TF-IDF & BM25-lite ====\n",
    "def compute_tfidf_vectors(job_texts, cv_texts):\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), min_df=2)\n",
    "    all_text = job_texts + cv_texts\n",
    "    X = vect.fit_transform(all_text)\n",
    "    return vect, X[:len(job_texts)], X[len(job_texts):]\n",
    "\n",
    "class BM25OkapiLite:\n",
    "    def __init__(self, corpus_tokens, k1=1.5, b=0.75):\n",
    "        self.k1 = k1; self.b = b\n",
    "        self.corpus = corpus_tokens\n",
    "        self.doc_freq = {}\n",
    "        self.doc_len = [len(d) for d in corpus_tokens]\n",
    "        self.avgdl = np.mean(self.doc_len) if self.doc_len else 0.0\n",
    "        for doc in corpus_tokens:\n",
    "            for w in set(doc):\n",
    "                self.doc_freq[w] = self.doc_freq.get(w, 0) + 1\n",
    "        self.N = len(corpus_tokens)\n",
    "    def idf(self, term):\n",
    "        n_qi = self.doc_freq.get(term, 0) + 0.5\n",
    "        return np.log((self.N - n_qi + 0.5) / n_qi + 1.0)\n",
    "    def score(self, query_tokens, index):\n",
    "        score = 0.0; doc = self.corpus[index]; dl = len(doc) or 1\n",
    "        for t in query_tokens:\n",
    "            f = doc.count(t)\n",
    "            if f == 0: continue\n",
    "            idf = self.idf(t)\n",
    "            denom = f + self.k1*(1 - self.b + self.b*dl/(self.avgdl + 1e-9))\n",
    "            score += idf * (f*(self.k1+1)) / denom\n",
    "        return score\n",
    "    def get_scores(self, query_tokens):\n",
    "        return np.array([self.score(query_tokens, i) for i in range(self.N)])\n",
    "\n",
    "def tokenize_simple(s: str): return normalize_text(s).split()\n",
    "\n",
    "job_texts = pairs[\"job_desc\"].tolist()\n",
    "cv_texts = pairs[\"cv_text\"].tolist()\n",
    "tfidf_vect, J_mat, C_mat = compute_tfidf_vectors(job_texts, cv_texts)\n",
    "feat_names = np.array(tfidf_vect.get_feature_names_out())\n",
    "\n",
    "def top_terms(matrix, feature_names, row_i, topk=12):\n",
    "    row = matrix[row_i]\n",
    "    if hasattr(row, \"toarray\"): row = row.toarray()\n",
    "    row = row.flatten()\n",
    "    idxs = np.argsort(-row)[:topk]\n",
    "    return set([feature_names[i] for i in idxs if row[i] > 0])\n",
    "\n",
    "job_terms_top = [top_terms(J_mat, feat_names, i, 12) for i in range(J_mat.shape[0])]\n",
    "cv_terms_top  = [top_terms(C_mat, feat_names, i, 12) for i in range(C_mat.shape[0])]\n",
    "\n",
    "bm25 = BM25OkapiLite([tokenize_simple(t) for t in cv_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== SBERT encode & FAISS build ====\n",
    "def sbert_encode(texts: List[str], model_name: str):\n",
    "    if not SBERT_OK:\n",
    "        print(\"[FALLBACK] Using TF-IDF cosine for semantic features.\")\n",
    "        vect = TfidfVectorizer(ngram_range=(1,2), min_df=2).fit(texts)\n",
    "        mat = vect.transform(texts).astype(\"float32\")\n",
    "        from sklearn.preprocessing import normalize\n",
    "        mat = normalize(mat)\n",
    "        return mat, None\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)\n",
    "    return np.asarray(emb, dtype=\"float32\"), model\n",
    "\n",
    "job_embeds, _ = sbert_encode(job_texts, SBERT_MODEL_NAME)\n",
    "cv_embeds, _  = sbert_encode(cv_texts, SBERT_MODEL_NAME)\n",
    "\n",
    "def build_faiss_index(vectors: np.ndarray):\n",
    "    if not FAISS_OK: return None\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "faiss_index = build_faiss_index(cv_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Assemble features (SBERT cosine) & labels ====\n",
    "if isinstance(job_embeds, np.ndarray) and isinstance(cv_embeds, np.ndarray) and job_embeds.shape[0]==len(pairs):\n",
    "    sem_cos = np.sum(job_embeds * cv_embeds, axis=1)\n",
    "else:\n",
    "    sem_cos = [float(cosine_similarity(J_mat[i], C_mat[i])[0,0]) for i in range(len(pairs))]\n",
    "\n",
    "pairs[\"feat_semantic_cosine\"] = sem_cos\n",
    "pairs[\"feat_keyword_overlap\"] = [keyword_overlap(job_terms_top[i], cv_terms_top[i]) for i in range(len(pairs))]\n",
    "pairs[\"feat_skill_jaccard\"] = [jaccard(set(js), set(cs)) for js,cs in zip(pairs[\"job_skills\"], pairs[\"cv_skills\"])]\n",
    "pairs[\"feat_group_weighted_skill\"] = [group_weighted_skill_score(set(js), set(cs)) for js,cs in zip(pairs[\"job_skills\"], pairs[\"cv_skills\"])]\n",
    "pairs[\"feat_exp_gap_penalty\"] = [exp_gap_penalty(req, got) for req,got in zip(pairs[\"job_reqexp\"], pairs[\"cv_exp\"])]\n",
    "pairs[\"feat_location_match\"] = (pairs[\"job_location\"] == pairs[\"cv_location\"]).astype(float)\n",
    "pairs[\"feat_industry_match\"] = (pairs[\"job_industry\"] == pairs[\"cv_industry\"]).astype(float)\n",
    "pairs[\"feat_bm25\"] = [bm25.get_scores(tokenize_simple(pairs[\"job_desc\"].iloc[i]))[i] for i in range(len(pairs))]\n",
    "\n",
    "pairs[\"cos_by_job_rank\"] = pairs.groupby(\"job_id\")[\"feat_semantic_cosine\"].rank(pct=True)\n",
    "pairs[\"label_gt2\"] = ((pairs[\"feat_keyword_overlap\"] > 0) & (pairs[\"cos_by_job_rank\"] >= 0.75)).astype(int)\n",
    "pairs[\"label\"] = ((pairs[\"label_gt2\"] == 1) | (pairs[\"label_gt1\"] == 1)).astype(int)\n",
    "pairs[[\"feat_semantic_cosine\",\"feat_keyword_overlap\",\"feat_skill_jaccard\",\"feat_bm25\",\"label\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ca04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Scoring functions ====\n",
    "def score_B1_IWF_Textrank(row):\n",
    "    return 0.6*row[\"feat_keyword_overlap\"] + 0.4*row[\"feat_semantic_cosine\"]\n",
    "def score_B2_Embedding(row):\n",
    "    return row[\"feat_semantic_cosine\"]\n",
    "def score_B3_JobVacancy(row, alpha=0.7, beta=0.3):\n",
    "    return alpha*row[\"feat_semantic_cosine\"] + beta*row[\"feat_keyword_overlap\"]\n",
    "def score_B4_KSA_SPLS(row):\n",
    "    return 0.6*row[\"feat_group_weighted_skill\"] + 0.4*row[\"feat_skill_jaccard\"]\n",
    "def score_B5_ResumeSummarizer(row):\n",
    "    return row[\"feat_semantic_cosine\"]\n",
    "def score_C6_Hybrid_Improved(row):\n",
    "    base = 0.5*row[\"feat_semantic_cosine\"] + 0.3*row[\"feat_skill_jaccard\"] + 0.2*row[\"feat_keyword_overlap\"]\n",
    "    penalty = 0.25*row[\"feat_exp_gap_penalty\"]\n",
    "    return base - penalty\n",
    "def score_C7_BM25_SkillWeight(row):\n",
    "    return 0.6*row[\"feat_bm25\"] + 0.4*row[\"feat_group_weighted_skill\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77006c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Metrics ====\n",
    "def precision_at_k(labels_sorted, k):\n",
    "    k = min(k, len(labels_sorted)); \n",
    "    return 0.0 if k==0 else sum(labels_sorted[:k]) / k\n",
    "def recall_at_k(labels_sorted, k):\n",
    "    tot = sum(labels_sorted); \n",
    "    if tot==0: return 0.0\n",
    "    k = min(k, len(labels_sorted)); \n",
    "    return sum(labels_sorted[:k]) / tot\n",
    "def average_precision(labels_sorted):\n",
    "    num_pos = sum(labels_sorted); \n",
    "    if num_pos==0: return 0.0\n",
    "    ap, hits = 0.0, 0\n",
    "    for i, y in enumerate(labels_sorted, 1):\n",
    "        if y==1: hits+=1; ap += hits / i\n",
    "    return ap / num_pos\n",
    "def reciprocal_rank(labels_sorted):\n",
    "    for i, y in enumerate(labels_sorted, 1):\n",
    "        if y==1: return 1.0/i\n",
    "    return 0.0\n",
    "def dcg_at_k(labels_sorted, k):\n",
    "    s = 0.0\n",
    "    for i, y in enumerate(labels_sorted[:k], 1):\n",
    "        s += (2**y - 1) / math.log2(i+1)\n",
    "    return s\n",
    "def ndcg_at_k(labels_sorted, k):\n",
    "    ideal = sorted(labels_sorted, reverse=True)\n",
    "    denom = dcg_at_k(ideal, k)\n",
    "    return 0.0 if denom==0 else dcg_at_k(labels_sorted, k)/denom\n",
    "def evaluate_ranking(group_df, score_col, k_list=[5,10]):\n",
    "    df = group_df.sort_values(by=score_col, ascending=False)\n",
    "    labels = df[\"label\"].astype(int).tolist()\n",
    "    metrics = {}\n",
    "    for k in k_list:\n",
    "        metrics[f\"P@{k}\"] = precision_at_k(labels, k)\n",
    "        metrics[f\"R@{k}\"] = recall_at_k(labels, k)\n",
    "        metrics[f\"nDCG@{k}\"] = ndcg_at_k(labels, k)\n",
    "    metrics[\"MAP\"] = average_precision(labels)\n",
    "    metrics[\"MRR\"] = reciprocal_rank(labels)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc310c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== FAISS retrieval + classifier + reranker ====\n",
    "FEATURE_COLS = [\n",
    "    \"feat_semantic_cosine\",\"feat_skill_jaccard\",\"feat_keyword_overlap\",\"feat_exp_gap_penalty\",\n",
    "    \"feat_industry_match\",\"feat_location_match\",\"feat_bm25\",\"feat_group_weighted_skill\"\n",
    "]\n",
    "\n",
    "def retrieve_candidates_alljobs(K=200):\n",
    "    # Query all jobs over the CV corpus to get candidate cv_id lists per row index.\n",
    "    if FAISS_OK and isinstance(cv_embeds, np.ndarray) and faiss_index is not None:\n",
    "        D, I = faiss_index.search(np.asarray(job_embeds, dtype=\"float32\"), min(K, cv_embeds.shape[0]))\n",
    "        return I.tolist()\n",
    "    # brute-force cosine similarity\n",
    "    def l2norm(x):\n",
    "        n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "        return x / n\n",
    "    if isinstance(job_embeds, np.ndarray) and isinstance(cv_embeds, np.ndarray):\n",
    "        Qn, Vn = l2norm(job_embeds), l2norm(cv_embeds)\n",
    "        sims = Qn @ Vn.T\n",
    "        return np.argsort(-sims, axis=1)[:, :min(K, Vn.shape[0])].tolist()\n",
    "    # fallback TF-IDF\n",
    "    out = []\n",
    "    for i in range(J_mat.shape[0]):\n",
    "        row = cosine_similarity(J_mat[i], C_mat).flatten()\n",
    "        order = np.argsort(-row)[:min(K, C_mat.shape[0])]\n",
    "        out.append(order.tolist())\n",
    "    return out\n",
    "\n",
    "def run_fold(train_df, test_df, k_list=[5,10], K_candidates=200):\n",
    "    X_tr = train_df[FEATURE_COLS].values; y_tr = train_df[\"label\"].values\n",
    "    scaler = MinMaxScaler().fit(X_tr)\n",
    "    clf = LogisticRegression(max_iter=1000).fit(scaler.transform(X_tr), y_tr)\n",
    "\n",
    "    # Candidate retrieval for all rows (then subset within each job at eval time)\n",
    "    cand_lists = retrieve_candidates_alljobs(K=K_candidates)\n",
    "\n",
    "    test_df = test_df.copy()\n",
    "    X_te = scaler.transform(test_df[FEATURE_COLS].values)\n",
    "    test_df[\"clf_proba\"] = clf.predict_proba(X_te)[:,1]\n",
    "    thr = 0.5  # tune on val if needed\n",
    "\n",
    "    methods = {\n",
    "        \"B1_IWF_TextRank\": lambda r: score_B1_IWF_Textrank(r),\n",
    "        \"B2_Embedding\": lambda r: score_B2_Embedding(r),\n",
    "        \"B3_JobVacancy\": lambda r: score_B3_JobVacancy(r),\n",
    "        \"B4_KSA_SPLS\": lambda r: score_B4_KSA_SPLS(r),\n",
    "        \"B5_ResumeSummarizer\": lambda r: score_B5_ResumeSummarizer(r),\n",
    "        \"C6_Hybrid_Improved\": lambda r: score_C6_Hybrid_Improved(r),\n",
    "        \"C7_BM25_SkillWeight\": lambda r: score_C7_BM25_SkillWeight(r),\n",
    "    }\n",
    "    reranker = LogisticRegression(max_iter=1000).fit(scaler.transform(X_tr), y_tr)\n",
    "\n",
    "    results = []\n",
    "    for name in list(methods.keys()) + [\"C8_ML_Reranker\"]:\n",
    "        metrics_agg = {f\"P@{k}\":[] for k in [5,10]}\n",
    "        for k in [5,10]: metrics_agg[f\"R@{k}\"] = []; metrics_agg[f\"nDCG@{k}\"] = []\n",
    "        MAP_list, MRR_list = [], []\n",
    "\n",
    "        for job_id, g in test_df.groupby(\"job_id\"):\n",
    "            # classifier filter\n",
    "            g = g[g[\"clf_proba\"] >= thr].copy()\n",
    "            if g.empty: continue\n",
    "\n",
    "            # restrict to FAISS candidates (union for rows of this job)\n",
    "            cand_union = set()\n",
    "            for pos in g.index.values:\n",
    "                top_cv_ids = [pairs.iloc[idx][\"cv_id\"] for idx in cand_lists[pos]]\n",
    "                cand_union.update(top_cv_ids)\n",
    "            g = g[g[\"cv_id\"].isin(cand_union)].copy()\n",
    "            if g.empty: continue\n",
    "\n",
    "            if name == \"C8_ML_Reranker\":\n",
    "                g[name] = reranker.predict_proba(scaler.transform(g[FEATURE_COLS].values))[:,1]\n",
    "            else:\n",
    "                g[name] = g.apply(methods[name], axis=1)\n",
    "\n",
    "            m = evaluate_ranking(g, score_col=name, k_list=[5,10])\n",
    "            for k in [5,10]:\n",
    "                metrics_agg[f\"P@{k}\"].append(m[f\"P@{k}\"])\n",
    "                metrics_agg[f\"R@{k}\"].append(m[f\"R@{k}\"])\n",
    "                metrics_agg[f\"nDCG@{k}\"].append(m[f\"nDCG@{k}\"])\n",
    "            MAP_list.append(m[\"MAP\"]); MRR_list.append(m[\"MRR\"])\n",
    "\n",
    "        row = {\"method\": name}\n",
    "        for k in [5,10]:\n",
    "            row[f\"P@{k}\"] = float(np.mean(metrics_agg[f\"P@{k}\"])) if metrics_agg[f\"P@{k}\"] else 0.0\n",
    "            row[f\"R@{k}\"] = float(np.mean(metrics_agg[f\"R@{k}\"])) if metrics_agg[f\"R@{k}\"] else 0.0\n",
    "            row[f\"nDCG@{k}\"] = float(np.mean(metrics_agg[f\"nDCG@{k}\"])) if metrics_agg[f\"nDCG@{k}\"] else 0.0\n",
    "        row[\"MAP\"] = float(np.mean(MAP_list)) if MAP_list else 0.0\n",
    "        row[\"MRR\"] = float(np.mean(MRR_list)) if MRR_list else 0.0\n",
    "        results.append(row)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf52c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 5-fold GroupKFold ====\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_summaries = []\n",
    "for fold_idx, (tr_idx, te_idx) in enumerate(gkf.split(pairs, groups=pairs[\"job_id\"])):\n",
    "    print(f\"Fold {fold_idx+1}/5\")\n",
    "    tr = pairs.iloc[tr_idx].reset_index(drop=True)\n",
    "    te = pairs.iloc[te_idx].reset_index(drop=True)\n",
    "    fold_res = run_fold(tr, te, k_list=[5,10], K_candidates=200)\n",
    "    fold_res[\"fold\"] = fold_idx+1\n",
    "    fold_summaries.append(fold_res)\n",
    "\n",
    "cv_results = pd.concat(fold_summaries, ignore_index=True)\n",
    "cv_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd070eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Aggregate & Save ====\n",
    "agg = cv_results.groupby(\"method\").agg({\n",
    "    \"P@5\":\"mean\",\"R@5\":\"mean\",\"nDCG@5\":\"mean\",\n",
    "    \"P@10\":\"mean\",\"R@10\":\"mean\",\"nDCG@10\":\"mean\",\n",
    "    \"MAP\":\"mean\",\"MRR\":\"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "agg = agg.sort_values(by=[\"MAP\",\"MRR\",\"nDCG@10\",\"P@5\"], ascending=False).reset_index(drop=True)\n",
    "display(agg)\n",
    "agg.to_csv(RESULTS_CSV_PATH, index=False)\n",
    "print(\"Saved summary to:\", RESULTS_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Plots ====\n",
    "plt.figure()\n",
    "plt.bar(agg[\"method\"], agg[\"MAP\"])\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.title(\"MAP by Method\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(agg[\"method\"], agg[\"MRR\"])\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.title(\"MRR by Method\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484f502",
   "metadata": {},
   "source": [
    "\n",
    "## Notes\n",
    "- Thay `SBERT_MODEL_NAME` để dùng các model mạnh hơn (VD: `BAAI/bge-m3`).  \n",
    "- FAISS dùng `IndexFlatIP` (cosine khi embedding đã normalize). Với dữ liệu lớn, chuyển IVF/PQ.  \n",
    "- **C6, C7, C8** đã có đủ. **C8** là ML reranker (LogReg) học từ toàn bộ feature.  \n",
    "- Có thể tách theo thời gian thay vì GroupKFold nếu dữ liệu có timestamp.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
